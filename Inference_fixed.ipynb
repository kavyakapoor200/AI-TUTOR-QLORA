{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovijC77pDMiv",
        "outputId": "b8d88a6a-0135-483d-b751-4432e687c713"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9nzqGK9EKPV"
      },
      "outputs": [],
      "source": [
        "mkdir -p /content/drive/MyDrive/AI_TUTOR_QLORA/{data,adapter,deploy}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAf3n6YKH7aM"
      },
      "outputs": [],
      "source": [
        "BASE_DIR = \"/content/drive/MyDrive/AI_TUTOR_QLORA\"\n",
        "\n",
        "DATA_DIR = f\"{BASE_DIR}/data\"\n",
        "ADAPTER_DIR = f\"{BASE_DIR}/adapter\"\n",
        "DEPLOY_DIR = f\"{BASE_DIR}/deploy\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_baDJrxYEaG5",
        "outputId": "7bb2b5de-4b5c-4dc1-a558-e1b41bd5e3d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "100  3947    0  3947    0     0   2067      0 --:--:--  0:00:01 --:--:--  2067\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0bin/micromamba\n",
            "100 5394k  100 5394k    0     0  1551k      0  0:00:03  0:00:03 --:--:-- 3584k\n"
          ]
        }
      ],
      "source": [
        "!curl -L https://micro.mamba.pm/api/micromamba/linux-64/1.5.8 \\\n",
        "  | tar -xvj bin/micromamba\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h91uZVfOEaJL",
        "outputId": "6eea8238-ff36-425f-f6a2-87cb13d293e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  3943    0  3943    0     0   3588      0 --:--:--  0:00:01 --:--:--  3588\n",
            "bin/micromamba\n",
            "100 5394k  100 5394k    0     0  2291k      0  0:00:02  0:00:02 --:--:-- 7207k\n"
          ]
        }
      ],
      "source": [
        "!curl -L https://micro.mamba.pm/api/micromamba/linux-64/1.5.8 \\\n",
        "  | tar -xvj bin/micromamba\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bezSvSMlEaLg",
        "outputId": "f0a1ee0f-0f6b-401d-bc40-9837a3c8fbfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bin/micromamba: ELF 64-bit LSB pie executable, x86-64, version 1 (GNU/Linux), dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2, for GNU/Linux 2.6.18, stripped\n"
          ]
        }
      ],
      "source": [
        "!file bin/micromamba\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bO8--DsREaPP",
        "outputId": "438c010c-2d64-4f4c-8c79-7cd1fcb28cbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m\u001b[1mwarning  libmamba\u001b[m 'root_prefix' set with default value: /root/micromamba\n",
            "\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.1s\n",
            "conda-forge/linux-64  \u28fe  \n",
            "conda-forge/noarch    \u28fe  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.2s\n",
            "conda-forge/linux-64   6%\n",
            "conda-forge/noarch     9%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.3s\n",
            "conda-forge/linux-64  19%\n",
            "conda-forge/noarch    37%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.4s\n",
            "conda-forge/linux-64  24%\n",
            "conda-forge/noarch    46%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.5s\n",
            "conda-forge/linux-64  33%\n",
            "conda-forge/noarch    64%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.6s\n",
            "conda-forge/linux-64  41%\n",
            "conda-forge/noarch    82%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gconda-forge/noarch                                \n",
            "[+] 0.7s\n",
            "conda-forge/linux-64  46%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.8s\n",
            "conda-forge/linux-64  52%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.9s\n",
            "conda-forge/linux-64  59%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.0s\n",
            "conda-forge/linux-64  68%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.1s\n",
            "conda-forge/linux-64  72%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.2s\n",
            "conda-forge/linux-64  74%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.3s\n",
            "conda-forge/linux-64  83%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.4s\n",
            "conda-forge/linux-64  92%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.5s\n",
            "conda-forge/linux-64  92%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.6s\n",
            "conda-forge/linux-64  92%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.7s\n",
            "conda-forge/linux-64  92%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.8s\n",
            "conda-forge/linux-64  92%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.9s\n",
            "conda-forge/linux-64  92%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.0s\n",
            "conda-forge/linux-64  92%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.1s\n",
            "conda-forge/linux-64  92%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.2s\n",
            "conda-forge/linux-64  92%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.3s\n",
            "conda-forge/linux-64  92%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.4s\n",
            "conda-forge/linux-64  92%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.5s\n",
            "conda-forge/linux-64  92%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.6s\n",
            "conda-forge/linux-64  92%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.7s\n",
            "conda-forge/linux-64  92%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.8s\n",
            "conda-forge/linux-64  92%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.9s\n",
            "conda-forge/linux-64  92%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.0s\n",
            "conda-forge/linux-64  92%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.1s\n",
            "conda-forge/linux-64  92%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.2s\n",
            "conda-forge/linux-64  92%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.3s\n",
            "conda-forge/linux-64  92%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.4s\n",
            "conda-forge/linux-64  92%\u001b[2K\u001b[1A\u001b[2K\u001b[0Gconda-forge/linux-64                              \n",
            "\u001b[?25h\n",
            "Transaction\n",
            "\n",
            "  Prefix: /root/micromamba/envs/qlora\n",
            "\n",
            "  Updating specs:\n",
            "\n",
            "   - python=3.10\n",
            "\n",
            "\n",
            "  Package              Version  Build                 Channel          Size\n",
            "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
            "  Install:\n",
            "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
            "\n",
            "  \u001b[32m+ tzdata          \u001b[0m     2025c  hc9c84f9_1            conda-forge     119kB\n",
            "  \u001b[32m+ ca-certificates \u001b[0m  2026.1.4  hbd8a1cb_0            conda-forge     147kB\n",
            "  \u001b[32m+ _libgcc_mutex   \u001b[0m       0.1  conda_forge           conda-forge       3kB\n",
            "  \u001b[32m+ libgomp         \u001b[0m    15.2.0  he0feb66_16           conda-forge     603kB\n",
            "  \u001b[32m+ _openmp_mutex   \u001b[0m       4.5  2_gnu                 conda-forge      24kB\n",
            "  \u001b[32m+ libgcc          \u001b[0m    15.2.0  he0feb66_16           conda-forge       1MB\n",
            "  \u001b[32m+ libstdcxx       \u001b[0m    15.2.0  h934c35e_16           conda-forge       6MB\n",
            "  \u001b[32m+ openssl         \u001b[0m     3.6.0  h26f9b46_0            conda-forge       3MB\n",
            "  \u001b[32m+ ncurses         \u001b[0m       6.5  h2d0b736_3            conda-forge     892kB\n",
            "  \u001b[32m+ libzlib         \u001b[0m     1.3.1  hb9d3cd8_2            conda-forge      61kB\n",
            "  \u001b[32m+ libgcc-ng       \u001b[0m    15.2.0  h69a702a_16           conda-forge      27kB\n",
            "  \u001b[32m+ libuuid         \u001b[0m    2.41.3  h5347b49_0            conda-forge      40kB\n",
            "  \u001b[32m+ libnsl          \u001b[0m     2.0.1  hb9d3cd8_1            conda-forge      34kB\n",
            "  \u001b[32m+ liblzma         \u001b[0m     5.8.1  hb9d3cd8_2            conda-forge     113kB\n",
            "  \u001b[32m+ libexpat        \u001b[0m     2.7.3  hecca717_0            conda-forge      77kB\n",
            "  \u001b[32m+ libffi          \u001b[0m     3.5.2  h9ec8514_0            conda-forge      58kB\n",
            "  \u001b[32m+ bzip2           \u001b[0m     1.0.8  hda65f42_8            conda-forge     260kB\n",
            "  \u001b[32m+ icu             \u001b[0m      78.1  h33c6efd_0            conda-forge      13MB\n",
            "  \u001b[32m+ readline        \u001b[0m       8.3  h853b02a_0            conda-forge     345kB\n",
            "  \u001b[32m+ zstd            \u001b[0m     1.5.7  hb78ec9c_6            conda-forge     601kB\n",
            "  \u001b[32m+ tk              \u001b[0m    8.6.13  noxft_ha0e22de_103    conda-forge       3MB\n",
            "  \u001b[32m+ libxcrypt       \u001b[0m    4.4.36  hd590300_1            conda-forge     100kB\n",
            "  \u001b[32m+ libsqlite       \u001b[0m    3.51.1  hf4e2dac_1            conda-forge     943kB\n",
            "  \u001b[32m+ ld_impl_linux-64\u001b[0m      2.45  default_hbd61a6d_105  conda-forge     731kB\n",
            "  \u001b[32m+ python          \u001b[0m   3.10.19  h3c07f61_2_cpython    conda-forge      25MB\n",
            "  \u001b[32m+ wheel           \u001b[0m    0.45.1  pyhd8ed1ab_1          conda-forge      63kB\n",
            "  \u001b[32m+ setuptools      \u001b[0m    80.9.0  pyhff2d567_0          conda-forge     749kB\n",
            "  \u001b[32m+ pip             \u001b[0m      25.3  pyh8b19718_0          conda-forge       1MB\n",
            "\n",
            "  Summary:\n",
            "\n",
            "  Install: 28 packages\n",
            "\n",
            "  Total download: 59MB\n",
            "\n",
            "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
            "\n",
            "\n",
            "\n",
            "Transaction starting\n",
            "\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\n",
            "Downloading        0%\n",
            "Extracting         0%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gtzdata                                             119.1kB @   1.2MB/s  0.1s\n",
            "[+] 0.1s\n",
            "Downloading  (5)   1%\n",
            "Extracting   (1)  \u28fe  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gca-certificates                                    146.5kB @   1.4MB/s  0.1s\n",
            "_openmp_mutex                                       23.6kB @ 221.6kB/s  0.1s\n",
            "_libgcc_mutex                                        2.6kB @  22.3kB/s  0.1s\n",
            "libgomp                                            603.3kB @   5.0MB/s  0.1s\n",
            "liblzma                                            112.9kB @ 765.6kB/s  0.0s\n",
            "libexpat                                            76.6kB @ 478.5kB/s  0.1s\n",
            "libgcc-ng                                           27.3kB @ 164.5kB/s  0.1s\n",
            "libsqlite                                          943.5kB @   5.7MB/s  0.0s\n",
            "[+] 0.2s\n",
            "Downloading  (5)   4%\n",
            "Extracting   (9)  \u28fe  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gwheel                                               62.9kB @ 311.8kB/s  0.1s\n",
            "libnsl                                              33.7kB @ 166.6kB/s  0.0s\n",
            "libgcc                                               1.0MB @   4.6MB/s  0.1s\n",
            "ncurses                                            891.6kB @   3.8MB/s  0.1s\n",
            "readline                                           345.1kB @   1.4MB/s  0.0s\n",
            "zstd                                               601.4kB @   2.3MB/s  0.2s\n",
            "setuptools                                         748.8kB @   2.6MB/s  0.1s\n",
            "libffi                                              57.8kB @ 197.1kB/s  0.1s\n",
            "[+] 0.3s\n",
            "Downloading  (5)  14%\n",
            "Extracting  (17)  \u28fe  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gopenssl                                              3.2MB @  10.0MB/s  0.1s\n",
            "libxcrypt                                          100.4kB @ 298.9kB/s  0.1s\n",
            "pip                                                  1.2MB @   3.2MB/s  0.1s\n",
            "ld_impl_linux-64                                   730.8kB @   1.9MB/s  0.1s\n",
            "[+] 0.4s\n",
            "Downloading  (5)  47%\n",
            "Extracting  (21)  \u28fe  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gicu                                                 12.7MB @  32.8MB/s  0.2s\n",
            "libuuid                                             40.3kB @ 100.5kB/s  0.1s\n",
            "libstdcxx                                            5.9MB @  14.3MB/s  0.1s\n",
            "libzlib                                             61.0kB @ 145.5kB/s  0.0s\n",
            "bzip2                                              260.3kB @ 594.1kB/s  0.0s\n",
            "tk                                                   3.3MB @   7.1MB/s  0.1s\n",
            "[+] 0.5s\n",
            "Downloading  (1)  72%\n",
            "Extracting  (27)  \u28fe  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.6s\n",
            "Downloading  (1)  87%\n",
            "Extracting  (25)  \u28fe  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.7s\n",
            "Downloading  (1)  90%\n",
            "Extracting  (25)  \u28fe  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gpython                                              25.3MB @  33.3MB/s  0.4s\n",
            "[+] 0.8s\n",
            "Downloading      100%\n",
            "Extracting  (26)  \u28fe  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.9s\n",
            "Downloading      100%\n",
            "Extracting  (24)  \u28fe  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.0s\n",
            "Downloading      100%\n",
            "Extracting  (24)  \u28fe  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.1s\n",
            "Downloading      100%\n",
            "Extracting  (24)  \u28fe  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.2s\n",
            "Downloading      100%\n",
            "Extracting  (23)  \u28fe  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.3s\n",
            "Downloading      100%\n",
            "Extracting  (23)  \u28fe  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.4s\n",
            "Downloading      100%\n",
            "Extracting  (21)  \u28fe  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.5s\n",
            "Downloading      100%\n",
            "Extracting  (21)  \u28fe  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.6s\n",
            "Downloading      100%\n",
            "Extracting  (21)  \u28fe  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.7s\n",
            "Downloading      100%\n",
            "Extracting  (19)  \u28fe  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.8s\n",
            "Downloading      100%\n",
            "Extracting  (19)  \u28fe  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.9s\n",
            "Downloading      100%\n",
            "Extracting  (17)  \u28fe  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.0s\n",
            "Downloading      100%\n",
            "Extracting  (17)  \u28fe  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.1s\n",
            "Downloading      100%\n",
            "Extracting  (17)  \u28fe  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.2s\n",
            "Downloading      100%\n",
            "Extracting  (17)  \u28fe  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.3s\n",
            "Downloading      100%\n",
            "Extracting  (16)  \u28fe  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.4s\n",
            "Downloading      100%\n",
            "Extracting  (16)  \u28fe  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.5s\n",
            "Downloading      100%\n",
            "Extracting  (15)  \u28fe  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.6s\n",
            "Downloading      100%\n",
            "Extracting  (15)  \u28fe  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.7s\n",
            "Downloading      100%\n",
            "Extracting  (15)  \u28fe  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.8s\n",
            "Downloading      100%\n",
            "Extracting  (14)  \u28fe  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.9s\n",
            "Downloading      100%\n",
            "Extracting  (14)  \u28fe  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.0s\n",
            "Downloading      100%\n",
            "Extracting  (14)  \u28fe  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.1s\n",
            "Downloading      100%\n",
            "Extracting  (12)  \u28fe  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.2s\n",
            "Downloading      100%\n",
            "Extracting  (12)  \u28fe  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.3s\n",
            "Downloading      100%\n",
            "Extracting  (11)  \u28fe  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.4s\n",
            "Downloading      100%\n",
            "Extracting  (10)  \u28fe  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.5s\n",
            "Downloading      100%\n",
            "Extracting  (10)  \u28fe  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.6s\n",
            "Downloading      100%\n",
            "Extracting   (8)  \u28fe  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.7s\n",
            "Downloading      100%\n",
            "Extracting   (8)  \u28fe  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.8s\n",
            "Downloading      100%\n",
            "Extracting   (8)  \u28fe  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.9s\n",
            "Downloading      100%\n",
            "Extracting   (8)  \u28fe  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 4.0s\n",
            "Downloading      100%\n",
            "Extracting   (6)  \u28fe  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 4.1s\n",
            "Downloading      100%\n",
            "Extracting   (6)  \u28fe  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 4.2s\n",
            "Downloading      100%\n",
            "Extracting   (6)  \u28fe  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 4.3s\n",
            "Downloading      100%\n",
            "Extracting   (6)  \u28fe  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 4.4s\n",
            "Downloading      100%\n",
            "Extracting   (6)  \u28fe  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 4.5s\n",
            "Downloading      100%\n",
            "Extracting   (4)  \u28fe  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 4.6s\n",
            "Downloading      100%\n",
            "Extracting   (4)  \u28fe  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 4.7s\n",
            "Downloading      100%\n",
            "Extracting   (4)  \u28fe  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 4.8s\n",
            "Downloading      100%\n",
            "Extracting   (4)  \u28fe  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 4.9s\n",
            "Downloading      100%\n",
            "Extracting   (3)  \u28fe  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 5.0s\n",
            "Downloading      100%\n",
            "Extracting   (2)  \u28fe  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 5.1s\n",
            "Downloading      100%\n",
            "Extracting   (2)  \u28fe  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 5.2s\n",
            "Downloading      100%\n",
            "Extracting   (2)  \u28fe  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 5.3s\n",
            "Downloading      100%\n",
            "Extracting       100%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G\u001b[?25hLinking tzdata-2025c-hc9c84f9_1\n",
            "Linking ca-certificates-2026.1.4-hbd8a1cb_0\n",
            "Linking _libgcc_mutex-0.1-conda_forge\n",
            "Linking libgomp-15.2.0-he0feb66_16\n",
            "Linking _openmp_mutex-4.5-2_gnu\n",
            "Linking libgcc-15.2.0-he0feb66_16\n",
            "Linking libstdcxx-15.2.0-h934c35e_16\n",
            "Linking openssl-3.6.0-h26f9b46_0\n",
            "Linking ncurses-6.5-h2d0b736_3\n",
            "Linking libzlib-1.3.1-hb9d3cd8_2\n",
            "Linking libgcc-ng-15.2.0-h69a702a_16\n",
            "Linking libuuid-2.41.3-h5347b49_0\n",
            "Linking libnsl-2.0.1-hb9d3cd8_1\n",
            "Linking liblzma-5.8.1-hb9d3cd8_2\n",
            "Linking libexpat-2.7.3-hecca717_0\n",
            "Linking libffi-3.5.2-h9ec8514_0\n",
            "Linking bzip2-1.0.8-hda65f42_8\n",
            "Linking icu-78.1-h33c6efd_0\n",
            "Linking readline-8.3-h853b02a_0\n",
            "Linking zstd-1.5.7-hb78ec9c_6\n",
            "Linking tk-8.6.13-noxft_ha0e22de_103\n",
            "Linking libxcrypt-4.4.36-hd590300_1\n",
            "Linking libsqlite-3.51.1-hf4e2dac_1\n",
            "Linking ld_impl_linux-64-2.45-default_hbd61a6d_105\n",
            "Linking python-3.10.19-h3c07f61_2_cpython\n",
            "Linking wheel-0.45.1-pyhd8ed1ab_1\n",
            "Linking setuptools-80.9.0-pyhff2d567_0\n",
            "Linking pip-25.3-pyh8b19718_0\n",
            "\n",
            "Transaction finished\n",
            "\n",
            "To activate this environment, use:\n",
            "\n",
            "    micromamba activate qlora\n",
            "\n",
            "Or to execute a single command in this environment, use:\n",
            "\n",
            "    micromamba run -n qlora mycommand\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!./bin/micromamba create -y -n qlora -c conda-forge python=3.10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SY3L1QloF3PP",
        "outputId": "b2887874-5439-4bd6-a11e-b6c35f2d097c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m\u001b[1mwarning  libmamba\u001b[m 'root_prefix' set with default value: /root/micromamba\n",
            "Python 3.10.19\n"
          ]
        }
      ],
      "source": [
        "!./bin/micromamba run -n qlora python --version\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sR2SlAWF3Rs",
        "outputId": "5ca5f7d8-fa00-4a4c-dd0b-d3069550c522"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m\u001b[1mwarning  libmamba\u001b[m 'root_prefix' set with default value: /root/micromamba\n",
            "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /packages/ea/2e/9d99c60771d275ecf6c914a612e9a577f740a615bc826bec132368e1d3ae/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux2014_x86_64.whl.metadata\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m\u001b[1mwarning  libmamba\u001b[m 'root_prefix' set with default value: /root/micromamba\n",
            "\u001b[33m\u001b[1mwarning  libmamba\u001b[m 'root_prefix' set with default value: /root/micromamba\n",
            "Requirement already satisfied: bitsandbytes==0.41.1 in /root/micromamba/envs/qlora/lib/python3.10/site-packages (0.41.1)\n",
            "\u001b[33m\u001b[1mwarning  libmamba\u001b[m 'root_prefix' set with default value: /root/micromamba\n",
            "\u001b[33m\u001b[1mwarning  libmamba\u001b[m 'root_prefix' set with default value: /root/micromamba\n"
          ]
        }
      ],
      "source": [
        "!./bin/micromamba run -n qlora pip install -q \\\n",
        "  torch==2.2.2+cu118 \\\n",
        "  torchvision==0.17.2+cu118 \\\n",
        "  torchaudio==2.2.2+cu118 \\\n",
        "  --extra-index-url https://download.pytorch.org/whl/cu118\n",
        "!./bin/micromamba run -n qlora pip install -q \\\n",
        "  transformers==4.36.2 \\\n",
        "  peft==0.7.1 \\\n",
        "  accelerate==0.25.0 \\\n",
        "  trl==0.7.10 \\\n",
        "  bitsandbytes==0.41.1 \\\n",
        "  datasets\n",
        "!./bin/micromamba run -n qlora pip install bitsandbytes==0.41.1\n",
        "!./bin/micromamba run -n qlora pip install -q \"numpy<2\"\n",
        "!./bin/micromamba run -n qlora pip install -q scipy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8B6-1hT_F3UR",
        "outputId": "039c55bb-9d8c-4ad7-b028-10d12f2f2446"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m\u001b[1mwarning  libmamba\u001b[m 'root_prefix' set with default value: /root/micromamba\n",
            "NumPy: 1.26.4\n",
            "SciPy: 1.15.3\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "AttributeError: module 'bitsandbytes' has no attribute '__version__'\n",
            "\u001b[33m\u001b[1mwarning  libmamba\u001b[m 'root_prefix' set with default value: /root/micromamba\n",
            "bitsandbytes import OK\n",
            "\u001b[33m\u001b[1mwarning  libmamba\u001b[m 'root_prefix' set with default value: /root/micromamba\n",
            "0.41.1\n"
          ]
        }
      ],
      "source": [
        "!./bin/micromamba run -n qlora python -c \"import numpy, scipy, bitsandbytes; print('NumPy:', numpy.__version__); print('SciPy:', scipy.__version__); print('bitsandbytes:', bitsandbytes.__version__); print('OK')\"\n",
        "!./bin/micromamba run -n qlora python -c \"import bitsandbytes; print('bitsandbytes import OK')\"\n",
        "!./bin/micromamba run -n qlora python -c \"import importlib.metadata as m; print(m.version('bitsandbytes'))\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243,
          "referenced_widgets": [
            "9dce015e8507470d934db0452daf6284",
            "f402933449b64f34a2b31a81116d9520",
            "8280ddd6f71046aa829c901909ff07c5",
            "434f49868c1a4e9e86416bd669d158ba",
            "69d0b9983e244201aa65dd5c77683d35",
            "d56cd02cc90e4455a07786879a9181d8",
            "21b4875e89ad402b847c2fc13d411dc0",
            "30b0deae663a49a8afbc3642343845ad",
            "d0f174e7240949f594c6390a7db7600d",
            "492e29357bd9467dbe4e6d171dbc2727",
            "2600d6e63591442988f0971fa6ddb2d5"
          ]
        },
        "id": "L2U2RX1iF3W3",
        "outputId": "1d018b23-6d82-4f58-c207-57026fcaa073"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9dce015e8507470d934db0452daf6284",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['custom_instruction', 'topic', 'model_name', 'model', 'skip_prompt_formatting', 'category', 'conversations', 'views', 'language', 'id', 'title', 'idx', 'hash', 'avatarUrl', 'system_prompt', 'source'],\n",
            "    num_rows: 800\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Stream the dataset (no full download)\n",
        "dataset = load_dataset(\n",
        "    \"teknium/OpenHermes-2.5\",\n",
        "    split=\"train\",\n",
        "    streaming=True\n",
        ")\n",
        "\n",
        "# Take only first 800 samples\n",
        "dataset_800 = dataset.take(800)\n",
        "\n",
        "# Convert to normal Dataset object\n",
        "from datasets import Dataset\n",
        "dataset_800 = Dataset.from_list(list(dataset_800))\n",
        "\n",
        "print(dataset_800)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITjZ4xqbF3Z3"
      },
      "outputs": [],
      "source": [
        "def format_chat(example):\n",
        "    tutor_system = (\n",
        "        \"You are an AI tutor. \"\n",
        "        \"Your job is to teach concepts clearly to beginners. \"\n",
        "        \"Always explain answers step by step using simple language \"\n",
        "        \"and examples when helpful.\"\n",
        "    )\n",
        "\n",
        "    convo = example[\"conversations\"]\n",
        "    text = tutor_system + \"\\n\\n\"\n",
        "\n",
        "    for turn in convo:\n",
        "        if turn[\"from\"] == \"human\":\n",
        "            text += f\"User: {turn['value'].strip()}\\n\"\n",
        "\n",
        "        elif turn[\"from\"] == \"gpt\":\n",
        "            text += \"Assistant: Let\u2019s understand this step by step.\\n\"\n",
        "            text += f\"{turn['value'].strip()}\\n\"\n",
        "\n",
        "    return {\"text\": text.strip()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "a1dff993b7d14e65870252b2b3684125",
            "92a6e65a44d048a7a3e69ae6c91dcbbe",
            "46ca2edd20c440508aab5a62caa039ae",
            "e8785edf961447188dbcd47611b7b756",
            "f08c957fef2f4e908eb4b4732f6ce2ce",
            "e0be52a2da8947c793975133bcb88815",
            "9ef3beabd5e2482888e8dcef8a634c2b",
            "8d569e0a11d541778b6d4e4c65965522",
            "3dfb4d7563714a168c577dd4fea3b560",
            "0a25261b0a584d04882be440973f900d",
            "11cb07c63e86454a8ffa4b05eed64de5"
          ]
        },
        "id": "kg2ZgcnvF3c3",
        "outputId": "3057f4d6-af90-4329-84f4-359100d84cef"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a1dff993b7d14e65870252b2b3684125",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['text'],\n",
            "    num_rows: 800\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "final_dataset = dataset_800.map(\n",
        "    format_chat,\n",
        "    remove_columns=dataset_800.column_names\n",
        ")\n",
        "\n",
        "print(final_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2Tas95PF3f6",
        "outputId": "67ffd696-842a-4ff9-8969-73c741f59cd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are an AI tutor. Your job is to teach concepts clearly to beginners. Always explain answers step by step using simple language and examples when helpful.\n",
            "\n",
            "User: Every day, a tree drops 7 leaves. How many leaves would it drop in a month of February in a non-leap year? Include your logic.\n",
            "Assistant: Let\u2019s understand this step by step.\n",
            "Here's the logic behind this:\n",
            "\n",
            "1. We know that February has 28 days in a non-leap year.\n",
            "2. If the tree drops 7 leaves every day, then over the course of February, it would drop:\n",
            "   Leaves dropped in February = Leaves per day * Days in February\n",
            "   = 7 leaves * 28 days\n",
            "   = 196 leaves\n",
            "\n",
            "So, the tree would drop 196 leaves in February in a non-leap year.\n",
            "------\n",
            "You are an AI tutor. Your job is to teach concepts clearly to beginners. Always explain answers step by step using simple language and examples when helpful.\n",
            "\n",
            "User: In analytical chemistry, what is the principle behind the use of an internal standard in quantitative analysis?\n",
            "A. It compensates for variations in sample preparation and instrumental response.\n",
            "B. It enhances the sensitivity of the analytical method.\n",
            "C. It reduces the detection limit of the analytical method.\n",
            "D. It increases the resolution between analyte peaks in chromatography.\n",
            "E. None of the above.\n",
            "Assistant: Let\u2019s understand this step by step.\n",
            "A. It compensates for variations in sample preparation and instrumental response.\n",
            "------\n",
            "You are an AI tutor. Your job is to teach concepts clearly to beginners. Always explain answers step by step using simple language and examples when helpful.\n",
            "\n",
            "User: A rectangular garden has a length of 25 feet and a width of 15 feet. If you want to build a fence around the entire garden, how many feet of fencing will you need?\n",
            "Assistant: Let\u2019s understand this step by step.\n",
            "To determine the amount of fencing needed, we first need to calculate the perimeter of the rectangle. The formula for the perimeter of a rectangle is P = 2(L+W), where L is the length and W is the width.\n",
            "\n",
            "Perimeter = 2(L+W)\n",
            "                 = 2(25 feet + 15 feet)\n",
            "                 = 2(40 feet)\n",
            "                 = 80 feet\n",
            "\n",
            "So, you will need 80 feet of fencing to enclose the entire garden.\n",
            "------\n",
            "You are an AI tutor. Your job is to teach concepts clearly to beginners. Always explain answers step by step using simple language and examples when helpful.\n",
            "\n",
            "User: What was the purpose of the Colosseum in Rome?\n",
            "Assistant: Let\u2019s understand this step by step.\n",
            "The Colosseum in Rome was used for various public spectacles and events. These included gladiatorial contests, mock sea battles, animal hunts, executions, re-enactments of famous battles, and dramas based on Classical mythology. It was a symbol of the grandeur and power of the Roman Empire and was a place for the emperor to connect with the people by providing them with entertainment and free food.\n",
            "------\n",
            "You are an AI tutor. Your job is to teach concepts clearly to beginners. Always explain answers step by step using simple language and examples when helpful.\n",
            "\n",
            "User: A man claims he can predict the outcome of a dice roll with 90% accuracy. If he predicts correctly on his first attempt, what is the probability that he actually has the ability to predict dice rolls with 90% accuracy? Assume there's a 1% chance that someone can have this ability.\n",
            "Assistant: Let\u2019s understand this step by step.\n",
            "Let's use Bayes' theorem again to solve this problem:\n",
            "\n",
            "Let A represent the event that the man actually has the ability to predict dice rolls with 90% accuracy, and C represent the event of predicting correctly on the first attempt.\n",
            "\n",
            "We want to find P(A|C), the probability that the man actually has the ability given that he predicted correctly on his first attempt.\n",
            "\n",
            "Bayes' theorem states that P(A|C) = P(C|A) * P(A) / P(C)\n",
            "\n",
            "First, let's find P(C|A): the probability of predicting correctly on the first attempt if the man actually has the ability. Since he claims 90% accuracy, this probability is 0.9.\n",
            "\n",
            "Next, let's find P(A): the probability that someone actually has the ability to predict dice rolls with 90% accuracy. We are told this is 1%, so P(A) = 0.01.\n",
            "\n",
            "Now we need to find P(C): the overall probability of predicting correctly on the first attempt. This can be calculated as the sum of probabilities for each case: P(C) = P(C|A) * P(A) + P(C|\u00acA) * P(\u00acA), where \u00acA represents not having the ability and P(\u00acA) = 1 - P(A) = 0.99.\n",
            "\n",
            "To find P(C|\u00acA), the probability of predicting correctly on the first attempt without the ability, we use the fact that there's a 1/6 chance of guessing correctly by random chance: P(C|\u00acA) = 1/6.\n",
            "\n",
            "So, P(C) = (0.9)*(0.01) + (1/6)*(0.99) = 0.009 + 0.165 = 0.174.\n",
            "\n",
            "Finally, we can calculate P(A|C) using Bayes' theorem:\n",
            "\n",
            "P(A|C) = P(C|A) * P(A) / P(C) = (0.9)*(0.01) / (0.174) \u2248 0.0517.\n",
            "\n",
            "Therefore, the probability that the man actually has the ability to predict dice rolls with 90% accuracy is approximately 5.17%.\n",
            "------\n"
          ]
        }
      ],
      "source": [
        "for i in range(5):\n",
        "    print(final_dataset[i][\"text\"])\n",
        "    print(\"------\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "c1468e28b19641eda9a053b632d53090",
            "55ac50c661d046fc84205a84a63e936b",
            "e57512bcc33b4b0785e4dd2d1fba5ee5",
            "5082c4d251be45b99d1b53f4ae68a17c",
            "259604c4f9f64dde801979ae05ac6af9",
            "12cb927c34b448e59e1c2d847eb13d44",
            "fdd17ab5bc7647e7a96b9a000aeb9436",
            "3323ec555d7c411883fdc02d57da00d2",
            "3dcb531e99184e2e970b41f57abce533",
            "8ba89f6cef114619b0cd2f39b32c9e0a",
            "269fdb0d73104742a108457454824cac"
          ]
        },
        "id": "LHt678I1F3ix",
        "outputId": "4eb04a4c-29f3-48f4-aa94-29634c9057d7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c1468e28b19641eda9a053b632d53090",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/800 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "final_dataset.save_to_disk(DATA_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uZockucJqcm",
        "outputId": "03817c27-b33a-4efd-e0b1-5d5e65104640"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m\u001b[1mwarning  libmamba\u001b[m 'root_prefix' set with default value: /root/micromamba\n",
            "\u001b[33m\u001b[1mwarning  libmamba\u001b[m 'root_prefix' set with default value: /root/micromamba\n"
          ]
        }
      ],
      "source": [
        "!./bin/micromamba run -n qlora pip install -q sentencepiece\n",
        "!./bin/micromamba run -n qlora pip install -q protobuf==3.20.3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-0Ug-s4Jqf9"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "cat << 'EOF' > load_mistral.py\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"CUDA:\", torch.version.cuda)\n",
        "print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "BASE_DIR = \"/content/drive/MyDrive/AI_TUTOR_QLORA/deploy\"\n",
        "\n",
        "tokenizer.save_pretrained(f\"{BASE_DIR}/base_tokenizer\")\n",
        "model.config.save_pretrained(f\"{BASE_DIR}/base_config\")\n",
        "\n",
        "print(\"\ud83d\udcbe Tokenizer & base config saved to Google Drive\")\n",
        "\n",
        "print(\"\u2705 MISTRAL 7B LOADED SUCCESSFULLY IN 4-BIT\")\n",
        "EOF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BZv2eHHJqju",
        "outputId": "aab9a3fc-1ac2-4bb9-ba2f-96afa780be15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m\u001b[1mwarning  libmamba\u001b[m 'root_prefix' set with default value: /root/micromamba\n",
            "/root/micromamba/envs/qlora/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "Torch: 2.2.2+cu118\n",
            "CUDA: 11.8\n",
            "GPU: Tesla T4\n",
            "/root/micromamba/envs/qlora/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/root/micromamba/envs/qlora/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/root/micromamba/envs/qlora/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "Downloading shards:   0% 0/2 [00:00<?, ?it/s]/root/micromamba/envs/qlora/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "\n",
            "model-00001-of-00002.safetensors:   0% 0.00/9.94G [00:00<?, ?B/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   0% 43.1k/9.94G [00:01<76:20:18, 36.2kB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   0% 9.69M/9.94G [00:01<20:30, 8.08MB/s]   \u001b[A\n",
            "model-00001-of-00002.safetensors:   0% 15.1M/9.94G [00:01<14:36, 11.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   0% 19.8M/9.94G [00:02<17:59, 9.19MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   0% 45.0M/9.94G [00:02<06:27, 25.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 70.1M/9.94G [00:02<03:44, 44.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 83.6M/9.94G [00:03<05:01, 32.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 121M/9.94G [00:04<04:02, 40.5MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 160M/9.94G [00:05<04:55, 33.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 227M/9.94G [00:08<05:56, 27.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 288M/9.94G [00:15<10:56, 14.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 352M/9.94G [00:16<07:14, 22.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 419M/9.94G [00:21<09:26, 16.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 481M/9.94G [00:22<06:38, 23.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 548M/9.94G [00:27<08:53, 17.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 615M/9.94G [00:28<06:23, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 682M/9.94G [00:33<08:06, 19.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 749M/9.94G [00:34<05:54, 25.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 816M/9.94G [00:34<04:23, 34.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 884M/9.94G [00:35<03:50, 39.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 950M/9.94G [00:38<04:17, 34.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 1.02G/9.94G [00:44<07:04, 21.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 1.08G/9.94G [00:44<05:09, 28.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 1.15G/9.94G [00:45<04:10, 35.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 1.21G/9.94G [00:50<06:08, 23.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 1.28G/9.94G [00:51<04:51, 29.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 1.35G/9.94G [00:56<06:44, 21.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 1.42G/9.94G [00:57<05:21, 26.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 1.48G/9.94G [01:02<06:55, 20.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 1.55G/9.94G [01:04<05:56, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 1.62G/9.94G [01:04<04:16, 32.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 1.68G/9.94G [01:05<03:07, 44.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 1.75G/9.94G [01:05<02:20, 58.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 1.82G/9.94G [01:05<01:44, 77.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 1.89G/9.94G [01:07<02:03, 65.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 2.22G/9.94G [01:07<00:40, 192MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 2.36G/9.94G [01:07<00:36, 210MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 2.42G/9.94G [01:07<00:34, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 2.49G/9.94G [01:08<00:33, 225MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 2.56G/9.94G [01:08<00:32, 225MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 2.63G/9.94G [01:08<00:33, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 2.69G/9.94G [01:09<00:34, 210MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 2.76G/9.94G [01:09<00:31, 231MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 2.85G/9.94G [01:13<02:01, 58.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 2.92G/9.94G [01:13<01:35, 73.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 2.98G/9.94G [01:14<01:32, 75.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 3.05G/9.94G [01:17<02:31, 45.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 3.10G/9.94G [01:18<02:43, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 3.17G/9.94G [01:19<02:00, 56.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 3.23G/9.94G [01:19<01:31, 73.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 3.30G/9.94G [01:25<04:03, 27.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 3.41G/9.94G [01:25<02:34, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 3.47G/9.94G [01:26<02:01, 53.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 3.54G/9.94G [01:31<03:49, 27.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 3.61G/9.94G [01:32<03:04, 34.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 3.67G/9.94G [01:35<03:38, 28.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 3.74G/9.94G [01:36<03:01, 34.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 3.81G/9.94G [01:36<02:13, 45.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 3.87G/9.94G [01:37<01:40, 60.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 3.94G/9.94G [01:37<01:16, 78.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 4.01G/9.94G [01:37<01:06, 88.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 4.08G/9.94G [01:38<00:49, 118MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 4.15G/9.94G [01:41<02:09, 44.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 4.21G/9.94G [01:42<01:37, 58.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 4.28G/9.94G [01:42<01:18, 72.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 4.35G/9.94G [01:43<01:13, 76.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 4.41G/9.94G [01:45<01:53, 48.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 4.49G/9.94G [01:49<02:53, 31.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 4.55G/9.94G [01:50<02:13, 40.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 4.62G/9.94G [01:56<03:50, 23.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 4.68G/9.94G [01:56<02:47, 31.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 4.75G/9.94G [02:00<03:22, 25.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 4.82G/9.94G [02:00<02:25, 35.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 4.88G/9.94G [02:06<03:53, 21.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 4.95G/9.94G [02:06<02:47, 29.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 5.04G/9.94G [02:10<02:59, 27.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 5.11G/9.94G [02:10<02:13, 36.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 5.18G/9.94G [02:16<03:33, 22.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 5.24G/9.94G [02:16<02:36, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 5.28G/9.94G [02:22<04:19, 17.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 5.35G/9.94G [02:22<02:55, 26.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 5.42G/9.94G [02:28<04:07, 18.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 5.48G/9.94G [02:29<03:05, 24.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 5.55G/9.94G [02:35<03:53, 18.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 5.62G/9.94G [02:36<03:12, 22.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 5.68G/9.94G [02:37<02:21, 30.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 5.75G/9.94G [02:41<02:52, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 5.82G/9.94G [02:45<03:14, 21.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 5.88G/9.94G [02:45<02:18, 29.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 5.98G/9.94G [02:51<02:58, 22.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 6.04G/9.94G [02:52<02:18, 28.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 6.11G/9.94G [02:55<02:32, 25.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 6.18G/9.94G [02:56<02:01, 31.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 6.25G/9.94G [03:02<02:54, 21.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 6.28G/9.94G [03:02<02:32, 24.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 6.35G/9.94G [03:05<02:35, 23.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 6.37G/9.94G [03:06<02:28, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 6.44G/9.94G [03:06<01:37, 35.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 6.46G/9.94G [03:06<01:28, 39.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 6.53G/9.94G [03:10<01:54, 29.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 6.59G/9.94G [03:10<01:19, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 6.66G/9.94G [03:11<01:08, 48.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 6.73G/9.94G [03:16<01:56, 27.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 6.79G/9.94G [03:17<01:45, 29.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 6.86G/9.94G [03:20<01:54, 27.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 6.93G/9.94G [03:21<01:21, 37.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 7.00G/9.94G [03:25<01:59, 24.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 7.06G/9.94G [03:26<01:26, 33.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 7.13G/9.94G [03:32<02:15, 20.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 7.20G/9.94G [03:33<01:47, 25.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 7.26G/9.94G [03:36<01:48, 24.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 7.33G/9.94G [03:36<01:17, 33.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 7.40G/9.94G [03:38<01:07, 37.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 7.46G/9.94G [03:38<00:48, 50.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 7.53G/9.94G [03:38<00:36, 66.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 7.60G/9.94G [03:38<00:27, 84.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 7.67G/9.94G [03:39<00:21, 108MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 7.73G/9.94G [03:39<00:16, 130MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 7.80G/9.94G [03:39<00:15, 138MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 7.86G/9.94G [03:40<00:12, 163MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 7.93G/9.94G [03:44<00:51, 39.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 8.00G/9.94G [03:45<00:38, 50.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 8.07G/9.94G [03:45<00:29, 62.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 8.13G/9.94G [03:48<00:45, 39.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 8.20G/9.94G [03:49<00:33, 51.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 8.27G/9.94G [03:55<01:06, 25.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 8.33G/9.94G [03:58<01:07, 23.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 8.40G/9.94G [03:59<00:52, 29.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 8.47G/9.94G [04:03<01:05, 22.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 8.53G/9.94G [04:05<00:52, 26.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 8.60G/9.94G [04:05<00:36, 37.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 8.67G/9.94G [04:05<00:25, 50.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 8.74G/9.94G [04:05<00:18, 66.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 8.80G/9.94G [04:06<00:13, 84.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 8.87G/9.94G [04:06<00:10, 107MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 8.94G/9.94G [04:06<00:07, 129MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 9.00G/9.94G [04:06<00:06, 153MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 9.07G/9.94G [04:07<00:04, 180MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 9.14G/9.94G [04:07<00:04, 191MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 9.21G/9.94G [04:07<00:03, 214MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 9.27G/9.94G [04:07<00:02, 229MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 9.34G/9.94G [04:08<00:02, 231MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 9.41G/9.94G [04:08<00:02, 224MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 9.47G/9.94G [04:08<00:02, 200MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 9.54G/9.94G [04:09<00:01, 233MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 9.61G/9.94G [04:09<00:01, 249MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 9.67G/9.94G [04:09<00:01, 236MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 9.74G/9.94G [04:13<00:03, 50.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 9.81G/9.94G [04:13<00:01, 67.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 9.88G/9.94G [04:13<00:00, 86.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 9.94G/9.94G [04:14<00:00, 39.1MB/s]\n",
            "Downloading shards:  50% 1/2 [04:14<04:14, 254.43s/it]\n",
            "model-00002-of-00002.safetensors:   0% 0.00/4.54G [00:00<?, ?B/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   0% 203k/4.54G [00:01<6:19:26, 199kB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   1% 67.2M/4.54G [00:01<01:15, 59.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   2% 91.9M/4.54G [00:02<01:50, 40.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   2% 100M/4.54G [00:02<02:12, 33.5MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:   4% 167M/4.54G [00:03<01:27, 50.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   5% 234M/4.54G [00:04<01:00, 70.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   7% 302M/4.54G [00:04<00:47, 89.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   8% 369M/4.54G [00:05<00:33, 124MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:  10% 436M/4.54G [00:09<01:50, 37.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  11% 519M/4.54G [00:09<01:20, 49.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  13% 586M/4.54G [00:15<02:31, 26.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  14% 628M/4.54G [00:15<02:01, 32.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  15% 671M/4.54G [00:17<02:14, 28.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  16% 723M/4.54G [00:19<02:09, 29.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  17% 773M/4.54G [00:19<01:38, 38.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  18% 840M/4.54G [00:20<01:14, 49.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  20% 891M/4.54G [00:20<01:09, 52.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  21% 958M/4.54G [00:23<01:32, 38.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  23% 1.03G/4.54G [00:23<01:07, 52.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  24% 1.09G/4.54G [00:24<00:49, 70.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  26% 1.16G/4.54G [00:24<00:37, 90.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  27% 1.23G/4.54G [00:24<00:30, 107MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:  28% 1.29G/4.54G [00:25<00:28, 112MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  30% 1.36G/4.54G [00:29<01:22, 38.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  31% 1.43G/4.54G [00:29<00:58, 53.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  33% 1.49G/4.54G [00:31<00:56, 53.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  34% 1.56G/4.54G [00:31<00:43, 68.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  36% 1.63G/4.54G [00:35<01:27, 33.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  37% 1.70G/4.54G [00:36<01:11, 39.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  39% 1.76G/4.54G [00:39<01:28, 31.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  39% 1.78G/4.54G [00:40<01:22, 33.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  41% 1.85G/4.54G [00:44<01:47, 25.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  42% 1.91G/4.54G [00:44<01:17, 33.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  44% 1.98G/4.54G [00:45<01:00, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  45% 2.05G/4.54G [00:50<01:37, 25.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  47% 2.11G/4.54G [00:52<01:27, 27.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  48% 2.18G/4.54G [00:52<01:01, 38.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  50% 2.25G/4.54G [00:53<00:56, 40.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  51% 2.32G/4.54G [00:54<00:40, 55.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  52% 2.38G/4.54G [00:54<00:31, 69.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  54% 2.45G/4.54G [00:54<00:24, 86.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  56% 2.52G/4.54G [01:00<01:05, 30.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  57% 2.59G/4.54G [01:00<00:46, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  58% 2.66G/4.54G [01:01<00:35, 52.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  60% 2.72G/4.54G [01:02<00:38, 47.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  61% 2.79G/4.54G [01:06<00:54, 31.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  63% 2.86G/4.54G [01:12<01:23, 20.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  65% 2.95G/4.54G [01:13<00:50, 31.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  66% 3.02G/4.54G [01:16<00:58, 25.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  68% 3.09G/4.54G [01:17<00:43, 33.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  69% 3.15G/4.54G [01:20<00:50, 27.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  71% 3.22G/4.54G [01:21<00:35, 37.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  72% 3.29G/4.54G [01:22<00:28, 43.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  74% 3.35G/4.54G [01:22<00:21, 56.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  75% 3.42G/4.54G [01:27<00:37, 29.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  77% 3.48G/4.54G [01:27<00:26, 39.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  78% 3.55G/4.54G [01:28<00:21, 46.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  80% 3.62G/4.54G [01:28<00:14, 61.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  81% 3.69G/4.54G [01:28<00:10, 80.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  83% 3.75G/4.54G [01:29<00:08, 98.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  84% 3.82G/4.54G [01:29<00:07, 95.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  85% 3.87G/4.54G [01:30<00:06, 105MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:  87% 3.94G/4.54G [01:30<00:04, 126MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  88% 4.00G/4.54G [01:30<00:03, 147MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  90% 4.07G/4.54G [01:30<00:02, 183MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  91% 4.14G/4.54G [01:31<00:02, 190MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  93% 4.21G/4.54G [01:35<00:07, 46.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  94% 4.27G/4.54G [01:35<00:04, 62.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  96% 4.34G/4.54G [01:41<00:07, 26.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  97% 4.41G/4.54G [01:41<00:03, 34.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  99% 4.47G/4.54G [01:45<00:02, 27.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors: 100% 4.54G/4.54G [01:46<00:00, 42.8MB/s]\n",
            "Downloading shards: 100% 2/2 [06:00<00:00, 180.43s/it]\n",
            "Loading checkpoint shards: 100% 2/2 [01:03<00:00, 31.59s/it]\n",
            "/root/micromamba/envs/qlora/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "generation_config.json: 100% 116/116 [00:00<00:00, 832kB/s]\n",
            "\ud83d\udcbe Tokenizer & base config saved to Google Drive\n",
            "\u2705 MISTRAL 7B LOADED SUCCESSFULLY IN 4-BIT\n"
          ]
        }
      ],
      "source": [
        "!./bin/micromamba run -n qlora python load_mistral.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWlhbAceJqnv",
        "outputId": "18be39f2-6bcb-4986-9f7e-ba04609aef3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m\u001b[1mwarning  libmamba\u001b[m 'root_prefix' set with default value: /root/micromamba\n",
            "\u001b[33m\u001b[1mwarning  libmamba\u001b[m 'root_prefix' set with default value: /root/micromamba\n",
            "/root/micromamba/envs/qlora/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/root/micromamba/envs/qlora/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/root/micromamba/envs/qlora/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "trl OK\n"
          ]
        }
      ],
      "source": [
        "!./bin/micromamba run -n qlora pip install -q trl==0.7.10\n",
        "!./bin/micromamba run -n qlora python -c \"import trl; print('trl OK')\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAAaZ9xkJqqt"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "cat << 'EOF' > train_qlora.py\n",
        "import torch\n",
        "from datasets import load_from_disk\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training\n",
        "from trl import SFTTrainer\n",
        "BASE_DIR = \"/content/drive/MyDrive/AI_TUTOR_QLORA\"\n",
        "DATA_DIR = f\"{BASE_DIR}/data\"\n",
        "ADAPTER_DIR = f\"{BASE_DIR}/adapter\"\n",
        "\n",
        "print(\"\ud83d\ude80 Starting QLoRA training\")\n",
        "\n",
        "# ------------------------\n",
        "# Load dataset\n",
        "# ------------------------\n",
        "final_dataset = load_from_disk(DATA_DIR)\n",
        "\n",
        "\n",
        "# ------------------------\n",
        "# Load model (4-bit)\n",
        "# ------------------------\n",
        "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "# \ud83d\udd25 REQUIRED FOR QLoRA\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# Memory savings\n",
        "model.gradient_checkpointing_enable()\n",
        "model.config.use_cache = False\n",
        "\n",
        "# ------------------------\n",
        "# LoRA configuration\n",
        "# ------------------------\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# ------------------------\n",
        "# Training arguments\n",
        "# ------------------------\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=ADAPTER_DIR,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    logging_steps=10,\n",
        "    save_steps=200,\n",
        "    save_total_limit=2,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# ------------------------\n",
        "# Trainer\n",
        "# ------------------------\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=final_dataset,\n",
        "    peft_config=lora_config,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    formatting_func=lambda x: x[\"text\"],\n",
        "    max_seq_length=512,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "print(\"\u2705 QLoRA fine-tuning completed\")\n",
        "EOF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9kvFRaeOTs_",
        "outputId": "06ee592d-cbba-4069-d485-a35dc7d1c29a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m\u001b[1mwarning  libmamba\u001b[m 'root_prefix' set with default value: /root/micromamba\n",
            "/root/micromamba/envs/qlora/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/root/micromamba/envs/qlora/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "\ud83d\ude80 Starting QLoRA training\n",
            "/root/micromamba/envs/qlora/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Loading checkpoint shards: 100% 2/2 [01:02<00:00, 31.26s/it]\n",
            "/root/micromamba/envs/qlora/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Map: 100% 800/800 [00:02<00:00, 324.82 examples/s]\n",
            "  0% 0/300 [00:00<?, ?it/s]/root/micromamba/envs/qlora/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 1.1829, 'learning_rate': 0.00019333333333333333, 'epoch': 0.1}\n",
            "{'loss': 0.8758, 'learning_rate': 0.0001866666666666667, 'epoch': 0.2}\n",
            "{'loss': 0.881, 'learning_rate': 0.00018, 'epoch': 0.3}\n",
            "{'loss': 0.851, 'learning_rate': 0.00017333333333333334, 'epoch': 0.4}\n",
            "{'loss': 0.8306, 'learning_rate': 0.0001666666666666667, 'epoch': 0.5}\n",
            "{'loss': 0.6878, 'learning_rate': 0.00016, 'epoch': 0.6}\n",
            "{'loss': 0.7855, 'learning_rate': 0.00015333333333333334, 'epoch': 0.7}\n",
            "{'loss': 0.7417, 'learning_rate': 0.00014666666666666666, 'epoch': 0.8}\n",
            "{'loss': 0.7227, 'learning_rate': 0.00014, 'epoch': 0.9}\n",
            "{'loss': 0.6772, 'learning_rate': 0.00013333333333333334, 'epoch': 1.0}\n",
            "{'loss': 0.6637, 'learning_rate': 0.00012666666666666666, 'epoch': 1.1}\n",
            "{'loss': 0.7737, 'learning_rate': 0.00012, 'epoch': 1.2}\n",
            "{'loss': 0.7251, 'learning_rate': 0.00011333333333333334, 'epoch': 1.3}\n",
            "{'loss': 0.6885, 'learning_rate': 0.00010666666666666667, 'epoch': 1.4}\n",
            "{'loss': 0.6841, 'learning_rate': 0.0001, 'epoch': 1.5}\n",
            "{'loss': 0.635, 'learning_rate': 9.333333333333334e-05, 'epoch': 1.6}\n",
            "{'loss': 0.6221, 'learning_rate': 8.666666666666667e-05, 'epoch': 1.7}\n",
            "{'loss': 0.6442, 'learning_rate': 8e-05, 'epoch': 1.8}\n",
            "{'loss': 0.669, 'learning_rate': 7.333333333333333e-05, 'epoch': 1.9}\n",
            "{'loss': 0.6833, 'learning_rate': 6.666666666666667e-05, 'epoch': 2.0}\n",
            " 67% 200/300 [57:25<27:52, 16.73s/it]/root/micromamba/envs/qlora/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 0.592, 'learning_rate': 6e-05, 'epoch': 2.1}\n",
            "{'loss': 0.6213, 'learning_rate': 5.333333333333333e-05, 'epoch': 2.2}\n",
            "{'loss': 0.6566, 'learning_rate': 4.666666666666667e-05, 'epoch': 2.3}\n",
            "{'loss': 0.6229, 'learning_rate': 4e-05, 'epoch': 2.4}\n",
            "{'loss': 0.6115, 'learning_rate': 3.3333333333333335e-05, 'epoch': 2.5}\n",
            "{'loss': 0.5723, 'learning_rate': 2.6666666666666667e-05, 'epoch': 2.6}\n",
            "{'loss': 0.6226, 'learning_rate': 2e-05, 'epoch': 2.7}\n",
            "{'loss': 0.6342, 'learning_rate': 1.3333333333333333e-05, 'epoch': 2.8}\n",
            "{'loss': 0.6447, 'learning_rate': 6.666666666666667e-06, 'epoch': 2.9}\n",
            "{'loss': 0.5282, 'learning_rate': 0.0, 'epoch': 3.0}\n",
            "{'train_runtime': 5169.2976, 'train_samples_per_second': 0.464, 'train_steps_per_second': 0.058, 'train_loss': 0.704366668065389, 'epoch': 3.0}\n",
            "100% 300/300 [1:26:09<00:00, 17.23s/it]\n",
            "\u2705 QLoRA fine-tuning completed\n"
          ]
        }
      ],
      "source": [
        "!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
        "!./bin/micromamba run -n qlora python train_qlora.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOMyzvCVJquH"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "cat << 'EOF' > save_adapter.py\n",
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "BASE_MODEL = \"mistralai/Mistral-7B-v0.1\"\n",
        "ADAPTER_PATH = \"/content/drive/MyDrive/AI_TUTOR_QLORA/adapter/checkpoint-200\"\n",
        "OUT_PATH = \"/content/drive/MyDrive/AI_TUTOR_QLORA/deploy/adapter\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    device_map=\"cpu\"\n",
        ")\n",
        "\n",
        "model = PeftModel.from_pretrained(model, ADAPTER_PATH)\n",
        "model.save_pretrained(OUT_PATH)\n",
        "\n",
        "print(\"\u2705 Final adapter saved for deployment\")\n",
        "EOF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7jQmkEOjXVl",
        "outputId": "15e33cb1-fb63-4cd6-d519-9d9f5aed1163"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m\u001b[1mwarning  libmamba\u001b[m 'root_prefix' set with default value: /root/micromamba\n",
            "/root/micromamba/envs/qlora/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/root/micromamba/envs/qlora/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/root/micromamba/envs/qlora/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Loading checkpoint shards:   0% 0/2 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "!./bin/micromamba run -n qlora python save_adapter.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFGvlDYEdK5q"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}